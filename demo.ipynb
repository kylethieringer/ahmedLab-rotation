{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demo 1\n",
    "\n",
    "Here we will do the following:  \n",
    "- load a nifty file\n",
    "- generate a fixed brain and use it for motion correction\n",
    "- cluster the pixels in each volume's slice\n",
    "- save the pixel-labels \n",
    "\n",
    "07 dec 2023  \n",
    "sama ahmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "from nre import io, moco, roi\n",
    "import ants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _zscore(x):\n",
    "    \"\"\"x: 2D array (n, t)\"\"\"\n",
    "\n",
    "    x_mean = np.mean(x, axis=-1)\n",
    "    x_std = np.std(x, axis=-1)\n",
    "    return (x - x_mean[:, None]) / x_std[:, None]\n",
    "\n",
    "def _zdff(F, win=200, smooth=False):\n",
    "    \"\"\"calculate zscored(df/f) based on F baseline activity\"\"\"\n",
    "\n",
    "    # find average signal in first `win` volumes\n",
    "    Fbase = np.mean(F[:, :win], axis=-1)\n",
    "    dff = (F - Fbase[:, None]) / Fbase[:, None]\n",
    "\n",
    "    if smooth:\n",
    "        dff = gaussian_filter1d(dff, sigma=1)\n",
    "    \n",
    "    return _zscore(dff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty_path = 'data/TSeries-12012023-1437-028_channel_1.nii'\n",
    "labels_path = 'data/demo_labels.h5'  # for saving the cluster assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load xyzt\n",
    "brain = io.load(nifty_path)\n",
    "\n",
    "n_slices = brain.shape[2]  # z\n",
    "n_vols = brain.shape[-1]  # t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate fixed brain\n",
    "\n",
    "This is typically made from the structural channel (e.g. red channel, tdtomato) but also fine to make it from the green, GCaMP channel. You simply average across time to get an xyz volume. If you have a long recording session, it's totally fine to use the first couple of minutes (or \"x\" volumetric timepoints) to generate the \"fixed mean brain\". e.g.  \n",
    "\n",
    "```\n",
    "# mean_brain based on the first 300 volumes\n",
    "mean_brain = np.mean(brain[0:300], axis=-1)\n",
    "fixed = ants.from_numpy(mean_brain)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_brain = np.mean(brain, axis=-1)\n",
    "fixed = ants.from_numpy(mean_brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the fixed brain as .nii if you want\n",
    "\n",
    "Note that `io.save` requires a numpy array so we'll use the `mean_brain` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io.save('data/fixed.nii', mean_brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## motion correction\n",
    "\n",
    "for each volume (aka timepoint), warp the volume to the fixed template using SyN transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "moco_brain = np.zeros_like(brain)\n",
    "\n",
    "for vol in range(n_vols):\n",
    "    moving = ants.from_numpy(brain[:, :, :, vol])\n",
    "    moco_brain[:, :, :, vol] = moco.apply(fixed, moving).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract ROIs\n",
    "\n",
    "There are lots of ways to extract ROIs. A simple approach is to cluster neighboring pixels in a brain slice based on their correlated activity patterns. Here we use hierarchical clustering to specify the clusters. This is essentially dimensionality reduction.  \n",
    "\n",
    "This approach is based on [Brezovec et al 2022](https://www.biorxiv.org/content/10.1101/2022.03.20.485047v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 100  # this can be any number... I suggest 500 or 1000. Brezovec et al used 2000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate clusters\n",
    "\n",
    "This can take a while...   \n",
    "One way to speed it up is by downsampling in time\n",
    "\n",
    "```\n",
    "ds = moco_brain[:, :, iSlice, 0:-1:5]  # every 5th timepoint\n",
    "cluster_model = roi.create_2d_clusters(ds, n_clusters, 'dat/cluster_mem')    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "# for each slice in Z, generate n_clusters of pixels and return the pixel label\n",
    "for iSlice in range(moco_brain.shape[2]):\n",
    "    cluster_model = roi.create_2d_clusters(moco_brain[:, :, iSlice, :], n_clusters, 'tmp/cluster_mem')    \n",
    "    labels.append(cluster_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the cluster labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File(labels_path, 'w')\n",
    "hf.create_dataset('labels', data=labels)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get zscored df/f for each cluster in each slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the baseline F window\n",
    "\n",
    "set the window we'll use to calculate the baseline fluorescence  \n",
    "\n",
    "Here it's set `n_vols` because our demo dataset is small. Depending on your experiment, `F_WINDOW` could be encompass the first couple of minutes of activity before stimulus turns on\n",
    "\n",
    "Alternatively, the `F_WINDOW` could be a few seconds before each stimulus, in which case you'll need to amend this code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_WINDOW = n_vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROIs = np.empty((n_slices, n_clusters, n_vols))\n",
    "\n",
    "for iSlice in range(n_slices):\n",
    "    mean_signal = np.empty(shape=(n_vols, n_clusters))\n",
    "\n",
    "    for vol in range(n_vols):\n",
    "        mean_supervox, _ = roi.get_supervoxel_mean_2D(moco_brain[:, :, iSlice, vol], labels[iSlice], n_clusters)\n",
    "        mean_signal[vol] = mean_supervox\n",
    "\n",
    "    # find zscored(df/f) and smooth over time\n",
    "    ROIs[iSlice, :, :] = _zdff(mean_signal.T, win=F_WINDOW, smooth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the zscored(df/f) values for all ROIs\n",
    "\n",
    "This has shape: n_slices, n_clusters, n_vols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.save('data/ROIs.nii', ROIs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nifty_roi_extractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
